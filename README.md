ğŸ–¼ï¸ Image Captioning using InceptionV3 + LSTM

Deep Learning project using Flickr8k Dataset

This project generates natural-language captions for images using:

InceptionV3 for image feature extraction

LSTM-based decoder for caption generation

TensorFlow/Keras deep learning framework

Streamlit web UI for inference

ğŸ“ Project Structure
evostra_captioning/
â”‚
â”œâ”€â”€ Flickr8k_text/
â”‚   â”œâ”€â”€ Flickr8k.token.txt
â”‚   â”œâ”€â”€ Flickr_8k.trainImages.txt
â”‚   â”œâ”€â”€ Flickr_8k.devImages.txt
â”‚   â”œâ”€â”€ Flickr_8k.testImages.txt
â”‚
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ *.npy     (InceptionV3 extracted features)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.h5
â”‚   â”œâ”€â”€ tokenizer.pkl
â”‚   â”œâ”€â”€ history.json
â”‚
â”œâ”€â”€ app.py        (Streamlit App)
â”œâ”€â”€ README.md     (This file)
â””â”€â”€ notebook.ipynb (Training Notebook - optional)

ğŸ“¦ Requirements

Install the dependencies:

pip install tensorflow==2.17.0
pip install numpy pillow
pip install streamlit
pip install h5py


If running in Google Colab, GPU is recommended.

ğŸ“¥ Dataset

Download Flickr8k Dataset (Images + Captions):

Images â†’ place them in:

Flickr8k_images/


Captions â†’ already provided as:

Flickr8k_text/Flickr8k.token.txt

ğŸ”§ Step 1 â€” Extract Image Features (InceptionV3)

Each image is passed through InceptionV3 and converted into a 2048-dimensional vector.

Features are saved as:

features/<image_name>.npy


This dramatically speeds up training.

ğŸ§¹ Step 2 â€” Clean Captions

Lowercase text

Remove punctuation

Add startseq/endseq tokens

Build tokenizer

Tokenizer is saved as:

tokenizer.pkl

ğŸ§  Step 3 â€” Train the Captioning Model

Model Architecture:

Input 1: 2048-dim feature vector

Input 2: caption tokens

Embedding layer

LSTM decoder

Dense softmax output

Training example:

model.fit(
    train_dataset,
    epochs=10,
    steps_per_epoch=2000,  # to reduce epoch time
    callbacks=[checkpoint]
)


Best model saved as:

best_model.h5

ğŸ¤ Step 4 â€” Generate Captions

After training:

model = load_model("best_model.h5")

with open("tokenizer.pkl", "rb") as f:
    tokenizer = pickle.load(f)

caption = generate_caption(model, tokenizer, feature_vector, max_length)

ğŸŒ Streamlit App

Run the app:

streamlit run app.py


Upload an image â†’ model generates a caption.

ğŸ§ª Example Output

Input Image:

Dog running in field

Model caption:

a brown dog running through the grass

ğŸ§© Future Improvements

Beam search for better captions

Add attention mechanism (Bahdanau/Luong)

Train on Flickr30k or MSCOCO

Convert to ONNX / TF Lite

ğŸ‘¤ Author

Om Yadav
Image Captioning Deep Learning Project
